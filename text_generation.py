# -*- coding: utf-8 -*-
"""Text_Generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iuvoYKathTmEiH4hP25Xustc3GPTGUnM

### Setting Up the Environment
"""

!pip install transformers
!pip install datasets
!pip install torch

import torch
#To Find GPU availability ?
print(torch.cuda.is_available())

#Upload Your Dataset to Colab
from google.colab import files
uploaded = files.upload()

"""###Tokenizing the Dataset."""

from transformers import GPT2Tokenizer

# Load GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Open and read the dataset file
with open("creative_professional_dataset.txt", "r") as file:
    text_data = file.read()

from transformers import GPT2Tokenizer

# Load GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Add a padding token to the tokenizer
tokenizer.pad_token = tokenizer.eos_token # Using eos_token as pad_token

# Open and read the dataset file
with open("creative_professional_dataset.txt", "r") as file:
    text_data = file.read()

# Tokenize the text
tokens = tokenizer(text_data, truncation=True, padding="max_length", max_length=128, return_tensors="pt")

import torch

# Save tokenized data to a file
torch.save(tokens, "tokenized_dataset.pt")

"""###Fine-tuning GPT-2"""

from transformers import GPT2LMHeadModel

# Load GPT-2 model
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Load tokenized dataset
tokens = torch.load("tokenized_dataset.pt")
train_data = tokens["input_ids"]

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=8,
    save_steps=500,
    save_total_limit=2,
)

from transformers import Trainer, DataCollatorForLanguageModeling

# Define a data collator for handling padding and masking
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False  # mlm=False for causal language modeling
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=data_collator,
    train_dataset=train_data,
)

# Train the model
trainer.train()

model.save_pretrained("./fine_tuned_gpt2")
tokenizer.save_pretrained("./fine_tuned_gpt2")

"""###Evaluating the Fine-Tuned Model"""

from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load the fine-tuned model
model = GPT2LMHeadModel.from_pretrained("./fine_tuned_gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("./fine_tuned_gpt2")

prompt = "To be a successful leader"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(
    inputs["input_ids"],
    max_length=100,  # Adjust length as needed
    num_return_sequences=1,  # Number of samples to generate
    temperature=0.7,  # Controls randomness (lower is more deterministic)
    top_k=50,  # Limits sampling to the top-k tokens
    top_p=0.95,  # Nucleus sampling
    repetition_penalty=1.2  # Penalizes repetition
)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)

"""###Deploying the Model"""

model.save_pretrained("./fine_tuned_gpt2")
tokenizer.save_pretrained("./fine_tuned_gpt2")

!pip install gradio

import gradio as gr
from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load the model and tokenizer
model = GPT2LMHeadModel.from_pretrained("./fine_tuned_gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("./fine_tuned_gpt2")

# Define a function to generate text
def generate_text(prompt):
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(inputs["input_ids"], max_length=100, temperature=0.7)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Create the Gradio interface
interface = gr.Interface(
    fn=generate_text,
    inputs="text",
    outputs="text",
    title="Creative Professional Aid Generator",
    description="Fine-tuned GPT-2 model to generate professional aid in a creative style."
)

# Launch the app
interface.launch()

from flask import Flask, request, jsonify
from transformers import GPT2LMHeadModel, GPT2Tokenizer

app = Flask(__name__)

# Load model and tokenizer
model = GPT2LMHeadModel.from_pretrained("./fine_tuned_gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("./fine_tuned_gpt2")

@app.route('/generate', methods=['POST'])
def generate():
    data = request.json
    prompt = data.get("prompt", "")
    inputs = tokenizer(prompt, return_tensors="pt")
    outputs = model.generate(inputs["input_ids"], max_length=100, temperature=0.7)
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return jsonify({"generated_text": result})

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)